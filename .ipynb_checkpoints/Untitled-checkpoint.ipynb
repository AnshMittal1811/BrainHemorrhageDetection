{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras import models, layers\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from capsnetKeras.capsulelayers import CapsuleLayer, PrimaryCap, Length, Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore  the warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('always')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# data visualisation and manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import seaborn as sns\n",
    " \n",
    "#configure\n",
    "# sets matplotlib to inline and displays graphs below the corressponding cell.\n",
    "%matplotlib inline  \n",
    "# style.use('fivethirtyeight')\n",
    "# sns.set(style='whitegrid',color_codes=True)\n",
    "\n",
    "#model selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,confusion_matrix,roc_curve,roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#preprocess.\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "#dl libraraies\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam,SGD,Adagrad,Adadelta,RMSprop\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "# specifically for cnn\n",
    "from keras.layers import Dropout, Flatten,Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    " \n",
    "import tensorflow as tf\n",
    "import random as rn\n",
    "\n",
    "# specifically for manipulating zipped images and getting numpy arrays of pixel values of images.\n",
    "import cv2                  \n",
    "import numpy as np  \n",
    "from tqdm import tqdm\n",
    "import os                   \n",
    "from random import shuffle  \n",
    "from zipfile import ZipFile\n",
    "from PIL import Image\n",
    "\n",
    "#TL pecific modules\n",
    "from keras.applications.vgg16 import VGG16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import random\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import keras.backend as K\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Dense, Flatten, Dropout, BatchNormalization\n",
    "from keras.layers import Conv2D, SeparableConv2D, MaxPool2D, LeakyReLU, Activation\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "import tensorflow as tf\n",
    "\n",
    "seed = 232\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = './Brain Tumor Images Dataset/'\n",
    "\n",
    "fig, ax = plt.subplots(2, 3, figsize=(15, 7))\n",
    "ax = ax.ravel()\n",
    "plt.tight_layout()\n",
    "\n",
    "for i, _set in enumerate(['training_set', 'test_set','validation_Set']):\n",
    "    set_path = input_path+_set\n",
    "    ax[i].imshow(plt.imread(set_path+'/hemmorhage_data/'+os.listdir(set_path+'/hemmorhage_data')[0]), cmap='gray')\n",
    "    ax[i].set_title('Set: {}, Condition: Positive'.format(_set))\n",
    "    ax[i+2].imshow(plt.imread(set_path+'/non_hemmorhage_data/'+os.listdir(set_path+'/non_hemmorhage_data')[1]), cmap='gray')\n",
    "    ax[i+2].set_title('Set: {}, Condition: Negative'.format(_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _set in ['training_set', 'test_set', 'validation_set']:\n",
    "    n_normal = len(os.listdir(input_path + _set + '/non_hemmorhage_data'))\n",
    "    n_infect = len(os.listdir(input_path + _set + '/hemmorhage_data'))\n",
    "    print('Set: {}, non_hemmorhage images: {}, hemmorhage images: {}'.format(_set, n_normal, n_infect))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras import models, layers\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from capsnetKeras.capsulelayers import CapsuleLayer, PrimaryCap, Length, Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(threshold=np.nan)\n",
    "\n",
    "number_of_classes = 2\n",
    "input_shape = (64, 64, 1)\n",
    "\n",
    "x = layers.Input(shape=input_shape)\n",
    "'''\n",
    "First layer is a convolutional layer with 100 × 7 × 7 filters and stride of 1 which leads to 64 feature maps of size 56×56.\n",
    "'''\n",
    "conv1 = layers.Conv2D(100, (3, 3), activation='relu', name=\"FirstLayer\")(x)\n",
    "'''\n",
    "The second layer is a Primary Capsule layer a from 128×7×7 convolutions with strides of 1.\n",
    "'''\n",
    "primaryCaps = PrimaryCap(inputs=conv1, dim_capsule=8, n_channels=16, kernel_size=3, strides=2, padding='valid')\n",
    "'''\n",
    "Final capsule layer includes 2 capsules, referred to as “Class Capsules,’ ’one for each of 2 classes. The dimension of these capsules is 16.\n",
    "'''\n",
    "capLayer2 = CapsuleLayer(num_capsule=2, dim_capsule=16, routings=8, name=\"ThirdLayer\")(primaryCaps)\n",
    "\n",
    "# Layer 4: This is an auxiliary layer to replace each capsule with its\n",
    "# length. Just to match the true label's shape.\n",
    "# If using tensorflow, this will not be necessary. :)\n",
    "out_caps = Length(name='capsnet')(capLayer2)\n",
    "\n",
    "\n",
    "# Decoder network.\n",
    "y = layers.Input(shape=(number_of_classes,))\n",
    "# The true label is used to mask the output of capsule layer. For training\n",
    "masked_by_y = Mask()([capLayer2, y])\n",
    "# Mask using the capsule with maximal length. For prediction\n",
    "masked = Mask()(capLayer2)\n",
    "\n",
    "\n",
    "# Shared Decoder model in training and prediction\n",
    "decoder = models.Sequential(name='decoder')\n",
    "decoder.add(layers.Dense(512, activation='relu',input_dim=16 * number_of_classes))\n",
    "decoder.add(layers.Dense(8192, activation='relu'))\n",
    "decoder.add(layers.Dense(np.prod(input_shape), activation='sigmoid'))\n",
    "decoder.add(layers.Reshape(target_shape=input_shape, name='out_recon'))\n",
    "\n",
    "# Models for training and evaluation (prediction)\n",
    "train_model = models.Model([x, y], [out_caps, decoder(masked_by_y)])\n",
    "\n",
    "\n",
    "print(train_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_generator(data_directory, batch_size=16):\n",
    "    train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "    image_resize_height = 64\n",
    "    image_resize_width = 64\n",
    "\n",
    "    generator = train_datagen.flow_from_directory(\n",
    "        data_directory,\n",
    "        color_mode='grayscale',\n",
    "        target_size=(image_resize_height, image_resize_width),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle = True)\n",
    "\n",
    "    while 1:\n",
    "        x_batch, y_batch = generator.next()\n",
    "        yield ([x_batch, y_batch], [y_batch, x_batch])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_generator(data_directory, batch_size=16):\n",
    "    train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "    image_resize_height = 64\n",
    "    image_resize_width = 64\n",
    "\n",
    "    generator = train_datagen.flow_from_directory(\n",
    "        data_directory,\n",
    "        color_mode='grayscale',\n",
    "        target_size=(image_resize_height, image_resize_width),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle = True)\n",
    "\n",
    "    while 1:\n",
    "        x_batch, y_batch = generator.next()\n",
    "        yield ([x_batch, y_batch], [y_batch, x_batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = './Brain Tumor Images Dataset/'\n",
    "\n",
    "train_data_directory = input_path+'training_set/'\n",
    "validation_data_directory = input_path+'validation_set/'\n",
    "bsize = 16\n",
    "\n",
    "train_generator = create_generator(train_data_directory, \n",
    "                                   batch_size=bsize)\n",
    "\n",
    "validation_generator = create_test_generator(validation_data_directory,\n",
    "                                        batch_size=bsize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "\n",
    "train_model.compile(optimizer='rmsprop',\n",
    "                    loss= 'mse',\n",
    "                    metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointer1 = ModelCheckpoint(filepath='./Models/CapsNet1.hdf5', \n",
    "                               verbose=1, \n",
    "                               monitor='val_capsnet_acc', \n",
    "                               mode='max',\n",
    "                               save_best_only=True) \n",
    "\n",
    "es2 = EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    mode='min',\n",
    "    patience=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "Found 40 images belonging to 2 classes.\n",
      "Found 143 images belonging to 2 classes.\n",
      "14/14 [==============================] - 16s 1s/step - loss: 0.4537 - capsnet_loss: 0.3650 - decoder_loss: 0.0887 - capsnet_acc: 0.5431 - decoder_acc: 0.1073 - val_loss: 0.3735 - val_capsnet_loss: 0.2968 - val_decoder_loss: 0.0767 - val_capsnet_acc: 0.4792 - val_decoder_acc: 0.0610\n",
      "\n",
      "Epoch 00001: val_capsnet_acc improved from -inf to 0.47917, saving model to ./Models/CapsNet1.hdf5\n",
      "Epoch 2/50\n",
      "14/14 [==============================] - 9s 670ms/step - loss: 0.3705 - capsnet_loss: 0.2919 - decoder_loss: 0.0786 - capsnet_acc: 0.5413 - decoder_acc: 0.1031 - val_loss: 0.4225 - val_capsnet_loss: 0.3359 - val_decoder_loss: 0.0866 - val_capsnet_acc: 0.4583 - val_decoder_acc: 0.0549\n",
      "\n",
      "Epoch 00002: val_capsnet_acc did not improve from 0.47917\n",
      "Epoch 3/50\n",
      "14/14 [==============================] - 9s 678ms/step - loss: 0.3356 - capsnet_loss: 0.2597 - decoder_loss: 0.0759 - capsnet_acc: 0.5738 - decoder_acc: 0.1087 - val_loss: 0.3013 - val_capsnet_loss: 0.2234 - val_decoder_loss: 0.0779 - val_capsnet_acc: 0.7159 - val_decoder_acc: 0.0596\n",
      "\n",
      "Epoch 00003: val_capsnet_acc improved from 0.47917 to 0.71591, saving model to ./Models/CapsNet1.hdf5\n",
      "Epoch 4/50\n",
      "14/14 [==============================] - 10s 679ms/step - loss: 0.3200 - capsnet_loss: 0.2429 - decoder_loss: 0.0771 - capsnet_acc: 0.6223 - decoder_acc: 0.1112 - val_loss: 0.2964 - val_capsnet_loss: 0.2161 - val_decoder_loss: 0.0803 - val_capsnet_acc: 0.7500 - val_decoder_acc: 0.0551\n",
      "\n",
      "Epoch 00004: val_capsnet_acc improved from 0.71591 to 0.75000, saving model to ./Models/CapsNet1.hdf5\n",
      "Epoch 5/50\n",
      "14/14 [==============================] - 11s 762ms/step - loss: 0.2762 - capsnet_loss: 0.2005 - decoder_loss: 0.0757 - capsnet_acc: 0.6816 - decoder_acc: 0.1168 - val_loss: 0.3027 - val_capsnet_loss: 0.2239 - val_decoder_loss: 0.0788 - val_capsnet_acc: 0.6458 - val_decoder_acc: 0.0628\n",
      "\n",
      "Epoch 00005: val_capsnet_acc did not improve from 0.75000\n",
      "Epoch 6/50\n",
      "14/14 [==============================] - 11s 802ms/step - loss: 0.2801 - capsnet_loss: 0.2041 - decoder_loss: 0.0760 - capsnet_acc: 0.7166 - decoder_acc: 0.0991 - val_loss: 0.3359 - val_capsnet_loss: 0.2609 - val_decoder_loss: 0.0750 - val_capsnet_acc: 0.5568 - val_decoder_acc: 0.0578\n",
      "\n",
      "Epoch 00006: val_capsnet_acc did not improve from 0.75000\n",
      "Epoch 7/50\n",
      "14/14 [==============================] - 11s 810ms/step - loss: 0.2686 - capsnet_loss: 0.1939 - decoder_loss: 0.0747 - capsnet_acc: 0.7575 - decoder_acc: 0.1135 - val_loss: 0.3153 - val_capsnet_loss: 0.2415 - val_decoder_loss: 0.0737 - val_capsnet_acc: 0.5938 - val_decoder_acc: 0.0560\n",
      "\n",
      "Epoch 00007: val_capsnet_acc did not improve from 0.75000\n",
      "Epoch 8/50\n",
      "14/14 [==============================] - 17s 1s/step - loss: 0.2697 - capsnet_loss: 0.1949 - decoder_loss: 0.0748 - capsnet_acc: 0.7303 - decoder_acc: 0.1118 - val_loss: 0.3163 - val_capsnet_loss: 0.2417 - val_decoder_loss: 0.0746 - val_capsnet_acc: 0.6667 - val_decoder_acc: 0.0647\n",
      "\n",
      "Epoch 00008: val_capsnet_acc did not improve from 0.75000\n",
      "Epoch 9/50\n",
      "14/14 [==============================] - 14s 980ms/step - loss: 0.2443 - capsnet_loss: 0.1722 - decoder_loss: 0.0721 - capsnet_acc: 0.7750 - decoder_acc: 0.1094 - val_loss: 0.3024 - val_capsnet_loss: 0.2267 - val_decoder_loss: 0.0757 - val_capsnet_acc: 0.5909 - val_decoder_acc: 0.0555\n",
      "\n",
      "Epoch 00009: val_capsnet_acc did not improve from 0.75000\n",
      "Epoch 10/50\n",
      "14/14 [==============================] - 13s 963ms/step - loss: 0.2457 - capsnet_loss: 0.1741 - decoder_loss: 0.0716 - capsnet_acc: 0.7577 - decoder_acc: 0.1067 - val_loss: 0.3485 - val_capsnet_loss: 0.2725 - val_decoder_loss: 0.0760 - val_capsnet_acc: 0.5521 - val_decoder_acc: 0.0616\n",
      "\n",
      "Epoch 00010: val_capsnet_acc did not improve from 0.75000\n",
      "Epoch 11/50\n",
      "14/14 [==============================] - 13s 957ms/step - loss: 0.2676 - capsnet_loss: 0.1972 - decoder_loss: 0.0704 - capsnet_acc: 0.7166 - decoder_acc: 0.1108 - val_loss: 0.2924 - val_capsnet_loss: 0.2177 - val_decoder_loss: 0.0747 - val_capsnet_acc: 0.6562 - val_decoder_acc: 0.0600\n",
      "\n",
      "Epoch 00011: val_capsnet_acc did not improve from 0.75000\n",
      "Epoch 12/50\n",
      "14/14 [==============================] - 13s 956ms/step - loss: 0.2419 - capsnet_loss: 0.1736 - decoder_loss: 0.0682 - capsnet_acc: 0.7494 - decoder_acc: 0.1112 - val_loss: 0.3353 - val_capsnet_loss: 0.2595 - val_decoder_loss: 0.0758 - val_capsnet_acc: 0.5341 - val_decoder_acc: 0.0592\n",
      "\n",
      "Epoch 00012: val_capsnet_acc did not improve from 0.75000\n",
      "Epoch 13/50\n",
      "14/14 [==============================] - 14s 975ms/step - loss: 0.2479 - capsnet_loss: 0.1813 - decoder_loss: 0.0666 - capsnet_acc: 0.7574 - decoder_acc: 0.1011 - val_loss: 0.3173 - val_capsnet_loss: 0.2428 - val_decoder_loss: 0.0745 - val_capsnet_acc: 0.5938 - val_decoder_acc: 0.0620\n",
      "\n",
      "Epoch 00013: val_capsnet_acc did not improve from 0.75000\n",
      "Epoch 14/50\n",
      "14/14 [==============================] - 11s 776ms/step - loss: 0.2204 - capsnet_loss: 0.1558 - decoder_loss: 0.0646 - capsnet_acc: 0.8116 - decoder_acc: 0.1094 - val_loss: 0.3123 - val_capsnet_loss: 0.2374 - val_decoder_loss: 0.0749 - val_capsnet_acc: 0.6042 - val_decoder_acc: 0.0548\n",
      "\n",
      "Epoch 00014: val_capsnet_acc did not improve from 0.75000\n",
      "Epoch 15/50\n",
      "14/14 [==============================] - 12s 889ms/step - loss: 0.2362 - capsnet_loss: 0.1740 - decoder_loss: 0.0621 - capsnet_acc: 0.7610 - decoder_acc: 0.0974 - val_loss: 0.3423 - val_capsnet_loss: 0.2690 - val_decoder_loss: 0.0734 - val_capsnet_acc: 0.5114 - val_decoder_acc: 0.0637\n",
      "\n",
      "Epoch 00015: val_capsnet_acc did not improve from 0.75000\n",
      "Epoch 16/50\n",
      "14/14 [==============================] - 16s 1s/step - loss: 0.2260 - capsnet_loss: 0.1619 - decoder_loss: 0.0642 - capsnet_acc: 0.8027 - decoder_acc: 0.1193 - val_loss: 0.4537 - val_capsnet_loss: 0.3799 - val_decoder_loss: 0.0737 - val_capsnet_acc: 0.4896 - val_decoder_acc: 0.0631\n",
      "\n",
      "Epoch 00016: val_capsnet_acc did not improve from 0.75000\n",
      "Epoch 17/50\n",
      "14/14 [==============================] - 14s 976ms/step - loss: 0.2351 - capsnet_loss: 0.1734 - decoder_loss: 0.0617 - capsnet_acc: 0.7697 - decoder_acc: 0.1096 - val_loss: 0.3102 - val_capsnet_loss: 0.2365 - val_decoder_loss: 0.0736 - val_capsnet_acc: 0.6146 - val_decoder_acc: 0.0594\n",
      "\n",
      "Epoch 00017: val_capsnet_acc did not improve from 0.75000\n",
      "Epoch 18/50\n",
      "14/14 [==============================] - 13s 962ms/step - loss: 0.2161 - capsnet_loss: 0.1554 - decoder_loss: 0.0607 - capsnet_acc: 0.8060 - decoder_acc: 0.1080 - val_loss: 0.3153 - val_capsnet_loss: 0.2447 - val_decoder_loss: 0.0706 - val_capsnet_acc: 0.6023 - val_decoder_acc: 0.0601\n",
      "\n",
      "Epoch 00018: val_capsnet_acc did not improve from 0.75000\n",
      "Epoch 19/50\n",
      "14/14 [==============================] - 14s 983ms/step - loss: 0.2197 - capsnet_loss: 0.1586 - decoder_loss: 0.0611 - capsnet_acc: 0.8164 - decoder_acc: 0.1163 - val_loss: 0.3268 - val_capsnet_loss: 0.2513 - val_decoder_loss: 0.0755 - val_capsnet_acc: 0.5833 - val_decoder_acc: 0.0567\n",
      "\n",
      "Epoch 00019: val_capsnet_acc did not improve from 0.75000\n",
      "Epoch 20/50\n",
      "14/14 [==============================] - 14s 975ms/step - loss: 0.2347 - capsnet_loss: 0.1757 - decoder_loss: 0.0590 - capsnet_acc: 0.7750 - decoder_acc: 0.1176 - val_loss: 0.3025 - val_capsnet_loss: 0.2264 - val_decoder_loss: 0.0761 - val_capsnet_acc: 0.6354 - val_decoder_acc: 0.0680\n",
      "\n",
      "Epoch 00020: val_capsnet_acc did not improve from 0.75000\n",
      "Epoch 21/50\n",
      "14/14 [==============================] - 14s 975ms/step - loss: 0.2353 - capsnet_loss: 0.1760 - decoder_loss: 0.0592 - capsnet_acc: 0.7569 - decoder_acc: 0.1124 - val_loss: 0.4194 - val_capsnet_loss: 0.3489 - val_decoder_loss: 0.0705 - val_capsnet_acc: 0.4205 - val_decoder_acc: 0.0568\n",
      "\n",
      "Epoch 00021: val_capsnet_acc did not improve from 0.75000\n",
      "Epoch 22/50\n",
      "14/14 [==============================] - 14s 979ms/step - loss: 0.2265 - capsnet_loss: 0.1676 - decoder_loss: 0.0590 - capsnet_acc: 0.7471 - decoder_acc: 0.1137 - val_loss: 0.3872 - val_capsnet_loss: 0.3191 - val_decoder_loss: 0.0681 - val_capsnet_acc: 0.4375 - val_decoder_acc: 0.0635\n",
      "\n",
      "Epoch 00022: val_capsnet_acc did not improve from 0.75000\n",
      "Epoch 23/50\n",
      "14/14 [==============================] - 14s 983ms/step - loss: 0.2066 - capsnet_loss: 0.1477 - decoder_loss: 0.0589 - capsnet_acc: 0.8208 - decoder_acc: 0.1011 - val_loss: 0.2914 - val_capsnet_loss: 0.2204 - val_decoder_loss: 0.0710 - val_capsnet_acc: 0.6250 - val_decoder_acc: 0.0627\n",
      "\n",
      "Epoch 00023: val_capsnet_acc did not improve from 0.75000\n",
      "Epoch 24/50\n",
      "14/14 [==============================] - 14s 973ms/step - loss: 0.2194 - capsnet_loss: 0.1609 - decoder_loss: 0.0585 - capsnet_acc: 0.7881 - decoder_acc: 0.1146 - val_loss: 0.3433 - val_capsnet_loss: 0.2700 - val_decoder_loss: 0.0733 - val_capsnet_acc: 0.5568 - val_decoder_acc: 0.0559\n",
      "\n",
      "Epoch 00024: val_capsnet_acc did not improve from 0.75000\n",
      "Epoch 25/50\n",
      "14/14 [==============================] - 14s 978ms/step - loss: 0.2083 - capsnet_loss: 0.1497 - decoder_loss: 0.0586 - capsnet_acc: 0.8116 - decoder_acc: 0.1125 - val_loss: 0.3808 - val_capsnet_loss: 0.3089 - val_decoder_loss: 0.0719 - val_capsnet_acc: 0.4167 - val_decoder_acc: 0.0613\n",
      "\n",
      "Epoch 00025: val_capsnet_acc did not improve from 0.75000\n",
      "Epoch 26/50\n",
      "14/14 [==============================] - 14s 980ms/step - loss: 0.2026 - capsnet_loss: 0.1451 - decoder_loss: 0.0575 - capsnet_acc: 0.8152 - decoder_acc: 0.1093 - val_loss: 0.3293 - val_capsnet_loss: 0.2550 - val_decoder_loss: 0.0744 - val_capsnet_acc: 0.5104 - val_decoder_acc: 0.0587\n",
      "\n",
      "Epoch 00026: val_capsnet_acc did not improve from 0.75000\n",
      "Epoch 27/50\n",
      "14/14 [==============================] - 14s 976ms/step - loss: 0.2148 - capsnet_loss: 0.1572 - decoder_loss: 0.0575 - capsnet_acc: 0.7959 - decoder_acc: 0.1074 - val_loss: 0.3008 - val_capsnet_loss: 0.2288 - val_decoder_loss: 0.0720 - val_capsnet_acc: 0.6818 - val_decoder_acc: 0.0626\n",
      "\n",
      "Epoch 00027: val_capsnet_acc did not improve from 0.75000\n",
      "Epoch 28/50\n",
      "14/14 [==============================] - 14s 980ms/step - loss: 0.2096 - capsnet_loss: 0.1522 - decoder_loss: 0.0573 - capsnet_acc: 0.7937 - decoder_acc: 0.1077 - val_loss: 0.3274 - val_capsnet_loss: 0.2544 - val_decoder_loss: 0.0729 - val_capsnet_acc: 0.5417 - val_decoder_acc: 0.0615\n",
      "\n",
      "Epoch 00028: val_capsnet_acc did not improve from 0.75000\n",
      "Epoch 29/50\n",
      "14/14 [==============================] - 20s 1s/step - loss: 0.2066 - capsnet_loss: 0.1502 - decoder_loss: 0.0564 - capsnet_acc: 0.7842 - decoder_acc: 0.1171 - val_loss: 0.3403 - val_capsnet_loss: 0.2688 - val_decoder_loss: 0.0715 - val_capsnet_acc: 0.5729 - val_decoder_acc: 0.0564\n",
      "\n",
      "Epoch 00029: val_capsnet_acc did not improve from 0.75000\n",
      "Epoch 30/50\n",
      "14/14 [==============================] - 13s 907ms/step - loss: 0.2084 - capsnet_loss: 0.1526 - decoder_loss: 0.0558 - capsnet_acc: 0.8032 - decoder_acc: 0.1122 - val_loss: 0.4086 - val_capsnet_loss: 0.3376 - val_decoder_loss: 0.0711 - val_capsnet_acc: 0.4205 - val_decoder_acc: 0.0630\n",
      "\n",
      "Epoch 00030: val_capsnet_acc did not improve from 0.75000\n",
      "Epoch 31/50\n",
      "14/14 [==============================] - 18s 1s/step - loss: 0.1921 - capsnet_loss: 0.1346 - decoder_loss: 0.0575 - capsnet_acc: 0.8331 - decoder_acc: 0.1084 - val_loss: 0.3167 - val_capsnet_loss: 0.2433 - val_decoder_loss: 0.0734 - val_capsnet_acc: 0.6354 - val_decoder_acc: 0.0594\n",
      "\n",
      "Epoch 00031: val_capsnet_acc did not improve from 0.75000\n",
      "Epoch 32/50\n",
      "14/14 [==============================] - 16s 1s/step - loss: 0.2037 - capsnet_loss: 0.1460 - decoder_loss: 0.0577 - capsnet_acc: 0.8298 - decoder_acc: 0.1049 - val_loss: 0.3265 - val_capsnet_loss: 0.2521 - val_decoder_loss: 0.0743 - val_capsnet_acc: 0.6146 - val_decoder_acc: 0.0621\n",
      "\n",
      "Epoch 00032: val_capsnet_acc did not improve from 0.75000\n",
      "Epoch 33/50\n",
      "14/14 [==============================] - 14s 972ms/step - loss: 0.2018 - capsnet_loss: 0.1449 - decoder_loss: 0.0569 - capsnet_acc: 0.8107 - decoder_acc: 0.1184 - val_loss: 0.3548 - val_capsnet_loss: 0.2820 - val_decoder_loss: 0.0728 - val_capsnet_acc: 0.5114 - val_decoder_acc: 0.0570\n",
      "\n",
      "Epoch 00033: val_capsnet_acc did not improve from 0.75000\n",
      "Epoch 34/50\n",
      "14/14 [==============================] - 11s 757ms/step - loss: 0.1993 - capsnet_loss: 0.1434 - decoder_loss: 0.0560 - capsnet_acc: 0.8289 - decoder_acc: 0.1121 - val_loss: 0.4276 - val_capsnet_loss: 0.3567 - val_decoder_loss: 0.0709 - val_capsnet_acc: 0.4062 - val_decoder_acc: 0.0526\n",
      "\n",
      "Epoch 00034: val_capsnet_acc did not improve from 0.75000\n",
      "Epoch 35/50\n",
      "14/14 [==============================] - 22s 2s/step - loss: 0.2138 - capsnet_loss: 0.1578 - decoder_loss: 0.0560 - capsnet_acc: 0.7789 - decoder_acc: 0.1083 - val_loss: 0.3179 - val_capsnet_loss: 0.2423 - val_decoder_loss: 0.0756 - val_capsnet_acc: 0.6562 - val_decoder_acc: 0.0706\n",
      "\n",
      "Epoch 00035: val_capsnet_acc did not improve from 0.75000\n",
      "Epoch 36/50\n",
      "14/14 [==============================] - 15s 1s/step - loss: 0.1886 - capsnet_loss: 0.1339 - decoder_loss: 0.0547 - capsnet_acc: 0.8339 - decoder_acc: 0.1166 - val_loss: 0.3948 - val_capsnet_loss: 0.3238 - val_decoder_loss: 0.0709 - val_capsnet_acc: 0.5000 - val_decoder_acc: 0.0569\n",
      "\n",
      "Epoch 00036: val_capsnet_acc did not improve from 0.75000\n",
      "Epoch 37/50\n",
      " 7/14 [==============>...............] - ETA: 6s - loss: 0.1783 - capsnet_loss: 0.1208 - decoder_loss: 0.0575 - capsnet_acc: 0.8482 - decoder_acc: 0.1080"
     ]
    }
   ],
   "source": [
    "hist = train_model.fit_generator(\n",
    "           train_generator, steps_per_epoch=14, \n",
    "           epochs=50, validation_data=validation_generator, \n",
    "           validation_steps=7, verbose = 1, callbacks=[checkpointer1, es2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
